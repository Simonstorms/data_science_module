# -*- coding: utf-8 -*-
"""TidyData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JPfs9rVTIMwyglhsEU_FmWFjhO3fcfnw

# Tidying the Spaceship Titanic

In this session, you will train a small Machine Learning model to predict which passengers on a spaceship are teleported into another dimension.

[www.kaggle.com/competitions/spaceship-titanic](https://www.kaggle.com/competitions/spaceship-titanic)

## Part 1: Missing Values, Categories adn Normalization

Split into three groups and work on the taks in one of the sections.
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt

df = pd.read_csv("spaceship_titanic_train.csv", index_col=0)
df.head(5)

"""### Missing Values

**Tasks:**

* plot the number of missing values
* which columns in the dataset need to be completed
* would you prefer to fill missing values with the mean or median?
* plot a histogram of the filled column
* what are pros and cons of subsampling
* give a recommendation for dealing with the missing data
"""

# count missing values
df.isna().sum()

# insert by fixed value
df["colname"].fillna(3)  # add inplace=True to write it back

# insert by group mean
group_means = df.groupby("colname2")["colname"].transform("mean")
df["colname"].fillna(group_means)  # add inplace=True to write it back

# subsampling
def subsample(group_df):
    n = group_df.shape[0]
    return group_df.dropna().sample(n, replace=True).values

sample_values = df.groupby("colname2")["colname"].transform(subsample)
df["colname"].fillna(sample_values)  # add inplace=True to write it back

"""### Numbers vs Categories

**Tasks:**

* plot the number of data points for each bins
* what are pros and cons of the two methods of binning?
* which columns in the dataset need to be converted from categories to numbers?
* compare factorization and one-hot-encoding. What advantages do they offer?
* give a recommendation for dealing with the categorical data
"""

# bins with equal size
df["bins"] = pd.cut(df["colname"], 4)

# bins with equal number of data points
df["qbins"] = pd.qcut(df["colname"], 4)

# factorizing / label encoding
values, labels = df["colname"].factorize()
df["factor"] = values

# one-hot encoding
one_hot = pd.get_dummies(df["colname"]).astype(int)
df = pd.concat([df, one_hot], axis=1)

"""### Normalizing and Indexing

**Tasks:**

* run `.describe()` on the normalized numerical columns
* explain the differences between the three types of normalization
* plot a histogram before and after normalization. Does it change?
* recommend a normalization method for some of the numerical columns
* how does a clean index help working with a DataFrame??
* what is a MultiIndex in pandas?
"""

# normalization method 1: min-max scaling
xrange = df["colname"].max() - df["colname"].min()
df["minmax"] = ( df["colname"] - df["colname"].min() ) / xrange

# normalization method 2: standard scaling
df["normal"] = (df["colname"] - df["colname"].mean()) / df["colname"].std()
df["normal"].describe().round(5)

# normalization method 3: logarithms
df["log"] = np.log(df["colname"])  # use np.log1p if the column contains zeros

# move a column into the index
df.set_index("colname")  # add inplace=True to write it back

# move the index into a column
df.reset_index()  # add inplace=True to write it back

# create a MultiIndex
df.set_index(["colname1", "colname2"])

"""## Part 2: Machine Learning"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df = df.dropna()  # we cannot have any missing data from this point!

X = df[["col1", "col2", "col3"]]   # select numerical input columns
y = df["Transported"]  # single target column

Xtrain, Xval, ytrain, yval = train_test_split(X, y, train_size=0.8, random_state=42)
Xtrain.shape, Xval.shape, ytrain.shape, yval.shape

model = DecisionTreeClassifier(max_depth=3)
model.fit(Xtrain, ytrain)

ypred_train = model.predict(Xtrain)
ypred_val = model.predict(Xval)

round(accuracy_score(ytrain, ypred_train), 4)

round(accuracy_score(yval, ypred_val), 4)

_ = plot_tree(model, filled=True)  # add feature_names=["col1", "col2", ...] for readability

